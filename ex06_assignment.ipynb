{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing - Summer Term 2023\n",
    "### Hochschule Karlsruhe\n",
    "### Lecturer: Prof. Dr. Jannik Strötgen\n",
    "### Thanks to: Jun.-Prof. Dr. Andreas Spitz and his tutors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You will learn about:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Text Similarity\n",
    "- Levenshtein (Edit) Distance\n",
    "- TF-IDF Vectors and Cosine Similarity\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remark:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You have **two weeks** until we discuss the solutions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Task 1 - Levenshtein (Edit) Distance (pen and paper) (10P):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1:\n",
    "Calculate the Levenshtein distance between words \n",
    "- \"signed\" and \"sealed\"\n",
    "\n",
    "Fill out the edit distance table like it is shown on the lecture slides. (In addition to the lecture, I will provide further information how to do that in more detail via ILIAS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshteinDistance(str1, str2):\n",
    "    m = len(str1)\n",
    "    n = len(str2)\n",
    "    d = [[i] for i in range(1, m + 1)]   # d matrix rows\n",
    "    d.insert(0, list(range(0, n + 1)))   # d matrix columns\n",
    "    for j in range(1, n + 1):\n",
    "        for i in range(1, m + 1):\n",
    "            if str1[i - 1] == str2[j - 1]:  \n",
    "                substitutionCost = 0\n",
    "            else:\n",
    "                substitutionCost = 1\n",
    "            d[i].insert(j, min(d[i - 1][j] + 1,\n",
    "                               d[i][j - 1] + 1,\n",
    "                               d[i - 1][j - 1] + substitutionCost))\n",
    "            \n",
    "    # printing the matrix\n",
    "    for l in d:\n",
    "        print(*l)\n",
    "\n",
    "    return d[-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levenshteinDistance(\"signed\",\"sealed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: \n",
    "What is the advantage and disadvantage of this method for measuring word similarity?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vorteile:\n",
    "- Sprachagnostisch, da die Levenshtein Distanz auf Buchstabenebene agiert, ist die Sprache irrelevant.\n",
    "- Algorit\n",
    "Nachteile:\n",
    "- Semantik wird nicht betrachtet, Wörter die Synonyme sind, können große Distanz aufweisen.\n",
    "- Komplexität O(n*m), bei langen Wörten suboptimal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Text Similarity (10P):\n",
    "\n",
    "### Part 1:\n",
    "Your task is to calculate pair-wise Cosine similarities \n",
    "between the first 1000 sentences of the 'debates' dataset. \n",
    "Calculate the similarities on sentence TF-IDF vectors. Plot the similarity values in a heatmap visualization. \n",
    "\n",
    "- Calculate the Cosine similarity and visualize the results without cleaning the data.\n",
    "\n",
    "- Calculate the Cosine similarity and visualize the results after cleaning the data (think of appropriate methods for data cleaning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% tt\n"
    }
   },
   "outputs": [],
   "source": [
    "# you can use the given libraries or choose different ones for the similarity calculation\n",
    "# use plotly library to create a heatmap visualization\n",
    "\n",
    "import json\n",
    "import nltk\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "plotly.offline.init_notebook_mode(connected=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# read debates\n",
    "\n",
    "with open('data/texts.json', 'r') as infile:\n",
    "    data = json.load(infile)\n",
    "\n",
    "content_debates = data['debates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "corpus = sent_tokenize(content_debates)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "def tokenize_sentence(sentence):\n",
    "    return word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenize_sentence,lowercase=False)\n",
    "tfidf = vectorizer.fit_transform(corpus[:1000])\n",
    "print(f\"Features: {vectorizer.get_feature_names_out()[:500]}\")\n",
    "print(True if \"the\" in vectorizer.get_feature_names_out() else False)\n",
    "print(f\"Shape: {tfidf.shape}\")\n",
    "pairwise_similarity = tfidf * tfidf.T \n",
    "arr = pairwise_similarity.toarray() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = px.imshow(arr)\n",
    "fig1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_descending_similar_sent(arr):\n",
    "    # diagonal always 1 because of same sentence, replace with NaN\n",
    "    \n",
    "    # remove lower half of matrix and diagonal, to remove duplicates \n",
    "    arr[np.arange(arr.shape[0])[:,None] > np.arange(arr.shape[1])] = np.nan\n",
    "    np.fill_diagonal(arr, np.nan)  \n",
    "    \n",
    "    n = arr.shape[1]\n",
    "    sorted_arr = np.argsort(arr, axis=None)[::-1].__divmod__(n)\n",
    "    # because of the nan values, sorting does not work as intented, when sorting descending NaN first values\n",
    "    sorted_similarity = list(zip(*np.roll(sorted_arr,-np.count_nonzero(np.isnan(arr)))))\n",
    "    return sorted_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_sim = get_descending_similar_sent(arr)\n",
    "for x in desc_sim[:50]:\n",
    "    print(corpus[x[0]],corpus[x[1]], x, arr[x[0]][x[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stemmer = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
    "# tokenize\n",
    "tokenized_sentence  = [word_tokenize(t.translate(str.maketrans('','',string.punctuation))) for t in corpus]\n",
    "\n",
    "# removing stop words\n",
    "filter_tokens = [[word for word in sentence if word not in stop_words]for sentence in tokenized_sentence]\n",
    "\n",
    "# stemming\n",
    "stemmed_tokens = [[stemmer.stem(word) for word in sentence ]for sentence in filter_tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_tokenizer(text):\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer=identity_tokenizer,lowercase=False)\n",
    "tfidf = vectorizer.fit_transform(filter_tokens[:1000])\n",
    "pairwise_similarity = tfidf * tfidf.T \n",
    "arr = pairwise_similarity.toarray() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = px.imshow(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_sim = get_descending_similar_sent(arr)\n",
    "for x in desc_sim[:50]:\n",
    "    print(corpus[x[0]],corpus[x[1]], x, filter_tokens[x[0]], filter_tokens[x[1]], arr[x[0]][x[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "\n",
    "#### Part 2:\n",
    "\n",
    "- Interpret the results. What kind of insights can you gain from the heatmap visualizations?\n",
    "- Ohne Säuberung der Daten mehr \"Rauschen\" in der Heatmap, gerade durch die Stopwords die eine hohe Häufigkeit haben.\n",
    "\n",
    "- How does data cleaning influence the similarity results? \n",
    "- Es gibt weniger Ähnlichkeiten und durch das löschen der Stopwords + Stemming wird teilweise zu viel Kontext entfernt, wodurch Sätze eine Ählichkeit von 1 bekommen, aber unterschiedliche Aussagen enthalten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='ff000000'>\\# TEXT SUBMISSION ANSWER HERE (Double click to edit) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Task 3 - Search Engine (10P):\n",
    "\n",
    "### Part 1:\n",
    "Read the critics for the first 200 movies from the 'rottentomatoes' dataset. \n",
    "First, create a document for each movie by concatenating their critic entries. \n",
    "Remove stop words, and compute TF-IDF vectors for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO - ADD YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2:\n",
    "Write a function for querying the data. \n",
    "Given a set of input terms as parameter, the function should remove stop words in the input, \n",
    "compute a TF-IDF vector of the query, and match it to all stored document vectors by using Cosine similarity. \n",
    "Rank all documents by Cosine similarity to the query and output the 10 most similar documents to the query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO - ADD YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3:\n",
    "Run a few queries on the data. Discuss the types of errors that your search engine makes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='ff000000'>\\# TEXT SUBMISSION ANSWER HERE (Double click to edit) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting your results:\n",
    "\n",
    "To submit your results, please:\n",
    "\n",
    "- save this file, i.e., `ex??_assignment.ipynb`.\n",
    "- if you reference any external files (e.g., images), please create a zip or rar archive and put the notebook files and all referenced files in there.\n",
    "- login to ILIAS and submit the `*.ipynb` or archive for the corresponding assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarks:**\n",
    "    \n",
    "- Do not copy any code from the Internet. In case you want to use publicly available code, please, add the reference to the respective code snippet.\n",
    "- Check your code compiles and executes, even after you have restarted the Kernel.\n",
    "- Submit your written solutions and the coding exercises within the provided spaces and not otherwise.\n",
    "- Write the names of your partner and your name in the top section."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
